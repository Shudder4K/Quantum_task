{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. datagen.py\n",
    "Клітинка 1: Імпорти та завантаження моделей\n",
    "python"
   ],
   "id": "e42e65841a1faf74"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.549537Z",
     "start_time": "2024-11-28T16:19:42.545004Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Імпортуємо необхідні бібліотеки, включаючи spaCy для роботи з NER.",
   "id": "3df8dbf554f14e11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Клітинка 2: Функція для створення CoNLL-формату",
   "id": "ca3b41cfb69a4b35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.561828Z",
     "start_time": "2024-11-28T16:19:42.557609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_conll_format(texts, entities):\n",
    "    dataset = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        labels = [\"O\"] * len(tokens)\n",
    "        for entity in entities:\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.startswith(entity):\n",
    "                    labels[i] = \"B-MOUNTAIN\" if labels[i] == \"O\" else \"I-MOUNTAIN\"\n",
    "        dataset.extend(zip(tokens, labels))\n",
    "        dataset.append((\"\", \"\"))\n",
    "    return dataset"
   ],
   "id": "f07ed900935db3de",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Функція створює розмічені дані в форматі CoNLL із текстів та заданих сутностей.\n",
    "\n",
    "Клітинка 3: Створення даних про гори"
   ],
   "id": "d038a472d75f7884"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.573743Z",
     "start_time": "2024-11-28T16:19:42.569784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mountains = [\"Everest\", \"Kilimanjaro\", \"Mont-Blanc\", \"Aconcagua\", \"Lhotse\", \"Nuptse\"]\n",
    "\n",
    "texts = [\n",
    "    f\"The mount {mountains[0]}, standing at an impressive 8,848 meters, {mountains[0]} is the highest mountain on Earth and a beacon for adventurers worldwide.\"\n",
    "    for mountain in mountains\n",
    "]\n",
    "\n",
    "dataset = generate_conll_format(texts, mountains)\n"
   ],
   "id": "44519b0115a58cae",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Створюємо список текстів із назвами гір і генеруємо розмітку.",
   "id": "785a71934ad8b68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Клітинка 4: Збереження датасету",
   "id": "e8d3add55749d4ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.619642Z",
     "start_time": "2024-11-28T16:19:42.615578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"ner_dataset.conll\", \"w\") as file:\n",
    "    for token, label in dataset:\n",
    "        file.write(f\"{token} {label}\\n\")\n"
   ],
   "id": "7894358154a01b2a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Зберігаємо датасет у форматі CoNLL у файл.\n",
   "id": "3559f2c0b9ccb348"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. mount.py",
   "id": "8360c83d1297cecb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Клітинка 1:  Імпорти та базові функції"
   ],
   "id": "c3a9c70a12fce75b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.631034Z",
     "start_time": "2024-11-28T16:19:42.627732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "243deac7a7ca5c6b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Імпортуємо бібліотеки для роботи з BERT і токенізацією.\n",
    "\n",
    "Клітинка 2: Функція для парсингу CoNLL-файлів"
   ],
   "id": "f0532a9ae48a1e55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.643392Z",
     "start_time": "2024-11-28T16:19:42.638133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_conll(file_path):\n",
    "    sentences, labels = [], []\n",
    "    current_sentence, current_labels = [], []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    labels.append(current_labels)\n",
    "                    current_sentence, current_labels = [], []\n",
    "            else:\n",
    "                token, tag = line.split()\n",
    "                current_sentence.append(token)\n",
    "                current_labels.append(tag)\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        labels.append(current_labels)\n",
    "    return sentences, labels\n"
   ],
   "id": "b83ef3efbc39a770",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Зчитуємо CoNLL-дані та зберігаємо токени та відповідні їм теги.\n",
    "\n",
    "Клітинка 3: Токенізація та вирівнювання міток"
   ],
   "id": "5b7b3330fd59d3c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.657624Z",
     "start_time": "2024-11-28T16:19:42.652115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(sentences, labels):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        sentences,\n",
    "        is_split_into_words=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    aligned_labels = []\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        aligned_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n"
   ],
   "id": "1db048396c0a3922",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Використовуємо токенайзер для вирівнювання токенів та міток.\n",
    "\n",
    "Клітинка 4: Підготовка датасету та токенайзера"
   ],
   "id": "b6033e7e0c854110"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:43.002500Z",
     "start_time": "2024-11-28T16:19:42.665889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"ner_dataset.conll\"\n",
    "sentences, labels = parse_conll(file_path)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tag2id = {\"O\": 0, \"B-MOUNTAIN\": 1}\n",
    "id2tag = {v: k for k, v in tag2id.items()}\n",
    "\n",
    "tokenized_data = tokenize_and_align_labels(sentences, labels)\n"
   ],
   "id": "d93269ccbda787ca",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Завантажуємо датасет, налаштовуємо токенайзер і створюємо мітки.\n",
    "\n",
    "Клітинка 5: Створення датасету для PyTorch"
   ],
   "id": "4d5ea5a45e031fd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:43.028009Z",
     "start_time": "2024-11-28T16:19:43.020965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = NERDataset(tokenize_and_align_labels(train_sentences, train_labels))\n",
    "val_dataset = NERDataset(tokenize_and_align_labels(val_sentences, val_labels))\n"
   ],
   "id": "a620d5df8fb9e558",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Розбиваємо дані на тренувальний і валідаційний датасети.\n",
    "\n",
    "Клітинка 6: Налаштування тренування"
   ],
   "id": "42563c86f8b475f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:44.942883Z",
     "start_time": "2024-11-28T16:19:43.042775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(tag2id))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ],
   "id": "3c1add46bc1c6106",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Misha\\PycharmProjects\\mountains\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\2908876682.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Налаштовуємо модель і параметри тренування.\n",
    "\n",
    "Клітинка 7: Тренування моделі"
   ],
   "id": "b5872b5e398058c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:33:57.147291Z",
     "start_time": "2024-11-28T16:19:44.958483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./bert-ner\")\n"
   ],
   "id": "69d06ccffa2175b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 14:04, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.318423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.250810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.197148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.146047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.102733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.070225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.047477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.031124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.019781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.012491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.008256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.006031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.004522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.003444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Тренуємо модель і зберігаємо її.\n",
    "\n",
    "Клітинка 8: Тестування та візуалізація"
   ],
   "id": "e011b78d129d8341"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:34:03.911819Z",
     "start_time": "2024-11-28T16:33:57.165327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy import displacy\n",
    "\n",
    "ner_model = pipeline(\"ner\", model=\"./bert-ner\", tokenizer=\"./bert-ner\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Mount Everest is the biggest mountain in the world.\"\n",
    "\n",
    "results = ner_model(text)\n",
    "\n",
    "# Підготовка даних для spaCy\n",
    "def convert_to_spacy_format(text, results):\n",
    "    entities = []\n",
    "    for entity in results:\n",
    "        start = entity[\"start\"]\n",
    "        end = entity[\"end\"]\n",
    "        label = entity[\"entity_group\"]\n",
    "        entities.append({\"start\": start, \"end\": end, \"label\": label})\n",
    "    return {\"text\": text, \"ents\": entities, \"title\": \"Named Entities\"}\n",
    "\n",
    "# Конвертуємо результати\n",
    "spacy_data = convert_to_spacy_format(text, results)\n",
    "\n",
    "# Візуалізація за допомогою spaCy displacy\n",
    "displacy.render(spacy_data, style=\"ent\", manual=True, jupyter=True)"
   ],
   "id": "7f29fb3f0f9274c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><h2 style=\"margin: 0\">Named Entities</h2>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mount\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LABEL_0</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Everest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LABEL_1</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    is the biggest mountain in the world.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LABEL_0</span>\n",
       "</mark>\n",
       "</div></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Використовуємо модель для розпізнавання сутностей і візуалізуємо їх.\n",
    "\n"
   ],
   "id": "203880fa1a828f62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
