{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. datagen.py\n",
    "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 1: –Ü–º–ø–æ—Ä—Ç–∏ —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π\n",
    "python"
   ],
   "id": "e42e65841a1faf74"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.549537Z",
     "start_time": "2024-11-28T16:19:42.545004Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "–Ü–º–ø–æ—Ä—Ç—É—î–º–æ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏, –≤–∫–ª—é—á–∞—é—á–∏ spaCy –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ NER.",
   "id": "3df8dbf554f14e11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 2: –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è CoNLL-—Ñ–æ—Ä–º–∞—Ç—É",
   "id": "ca3b41cfb69a4b35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.561828Z",
     "start_time": "2024-11-28T16:19:42.557609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_conll_format(texts, entities):\n",
    "    dataset = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        labels = [\"O\"] * len(tokens)\n",
    "        for entity in entities:\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.startswith(entity):\n",
    "                    labels[i] = \"B-MOUNTAIN\" if labels[i] == \"O\" else \"I-MOUNTAIN\"\n",
    "        dataset.extend(zip(tokens, labels))\n",
    "        dataset.append((\"\", \"\"))\n",
    "    return dataset"
   ],
   "id": "f07ed900935db3de",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–§—É–Ω–∫—Ü—ñ—è —Å—Ç–≤–æ—Ä—é—î —Ä–æ–∑–º—ñ—á–µ–Ω—ñ –¥–∞–Ω—ñ –≤ —Ñ–æ—Ä–º–∞—Ç—ñ CoNLL —ñ–∑ —Ç–µ–∫—Å—Ç—ñ–≤ —Ç–∞ –∑–∞–¥–∞–Ω–∏—Ö —Å—É—Ç–Ω–æ—Å—Ç–µ–π.\n",
    "\n",
    "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 3: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –ø—Ä–æ –≥–æ—Ä–∏"
   ],
   "id": "d038a472d75f7884"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.573743Z",
     "start_time": "2024-11-28T16:19:42.569784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mountains = [\"Everest\", \"Kilimanjaro\", \"Mont-Blanc\", \"Aconcagua\", \"Lhotse\", \"Nuptse\"]\n",
    "\n",
    "texts = [\n",
    "    f\"The mount {mountains[0]}, standing at an impressive 8,848 meters, {mountains[0]} is the highest mountain on Earth and a beacon for adventurers worldwide.\"\n",
    "    for mountain in mountains\n",
    "]\n",
    "\n",
    "dataset = generate_conll_format(texts, mountains)\n"
   ],
   "id": "44519b0115a58cae",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "–°—Ç–≤–æ—Ä—é—î–º–æ —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç—ñ–≤ —ñ–∑ –Ω–∞–∑–≤–∞–º–∏ –≥—ñ—Ä —ñ –≥–µ–Ω–µ—Ä—É—î–º–æ —Ä–æ–∑–º—ñ—Ç–∫—É.",
   "id": "785a71934ad8b68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 4: –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É",
   "id": "e8d3add55749d4ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.619642Z",
     "start_time": "2024-11-28T16:19:42.615578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"ner_dataset.conll\", \"w\") as file:\n",
    "    for token, label in dataset:\n",
    "        file.write(f\"{token} {label}\\n\")\n"
   ],
   "id": "7894358154a01b2a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "–ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç —É —Ñ–æ—Ä–º–∞—Ç—ñ CoNLL —É —Ñ–∞–π–ª.\n",
   "id": "3559f2c0b9ccb348"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. mount.py",
   "id": "8360c83d1297cecb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 1:  –Ü–º–ø–æ—Ä—Ç–∏ —Ç–∞ –±–∞–∑–æ–≤—ñ —Ñ—É–Ω–∫—Ü—ñ—ó"
   ],
   "id": "c3a9c70a12fce75b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.631034Z",
     "start_time": "2024-11-28T16:19:42.627732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "243deac7a7ca5c6b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–Ü–º–ø–æ—Ä—Ç—É—î–º–æ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ BERT —ñ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—î—é.\n",
    "\n",
    "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 2: –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É CoNLL-—Ñ–∞–π–ª—ñ–≤"
   ],
   "id": "f0532a9ae48a1e55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.643392Z",
     "start_time": "2024-11-28T16:19:42.638133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_conll(file_path):\n",
    "    sentences, labels = [], []\n",
    "    current_sentence, current_labels = [], []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    labels.append(current_labels)\n",
    "                    current_sentence, current_labels = [], []\n",
    "            else:\n",
    "                token, tag = line.split()\n",
    "                current_sentence.append(token)\n",
    "                current_labels.append(tag)\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        labels.append(current_labels)\n",
    "    return sentences, labels\n"
   ],
   "id": "b83ef3efbc39a770",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–ó—á–∏—Ç—É—î–º–æ CoNLL-–¥–∞–Ω—ñ —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç–æ–∫–µ–Ω–∏ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ —ó–º —Ç–µ–≥–∏.\n",
    "\n",
    "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 3: –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è —Ç–∞ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è –º—ñ—Ç–æ–∫"
   ],
   "id": "5b7b3330fd59d3c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:42.657624Z",
     "start_time": "2024-11-28T16:19:42.652115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(sentences, labels):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        sentences,\n",
    "        is_split_into_words=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    aligned_labels = []\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        aligned_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n"
   ],
   "id": "1db048396c0a3922",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –¥–ª—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è —Ç–æ–∫–µ–Ω—ñ–≤ —Ç–∞ –º—ñ—Ç–æ–∫.\n",
    "\n",
    "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 4: –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç—É —Ç–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞"
   ],
   "id": "b6033e7e0c854110"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:43.002500Z",
     "start_time": "2024-11-28T16:19:42.665889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"ner_dataset.conll\"\n",
    "sentences, labels = parse_conll(file_path)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tag2id = {\"O\": 0, \"B-MOUNTAIN\": 1}\n",
    "id2tag = {v: k for k, v in tag2id.items()}\n",
    "\n",
    "tokenized_data = tokenize_and_align_labels(sentences, labels)\n"
   ],
   "id": "d93269ccbda787ca",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç, –Ω–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä —ñ —Å—Ç–≤–æ—Ä—é—î–º–æ –º—ñ—Ç–∫–∏.\n",
    "\n",
    "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 5: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è PyTorch"
   ],
   "id": "4d5ea5a45e031fd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:43.028009Z",
     "start_time": "2024-11-28T16:19:43.020965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
    "\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    sentences, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = NERDataset(tokenize_and_align_labels(train_sentences, train_labels))\n",
    "val_dataset = NERDataset(tokenize_and_align_labels(val_sentences, val_labels))\n"
   ],
   "id": "a620d5df8fb9e558",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–†–æ–∑–±–∏–≤–∞—î–º–æ –¥–∞–Ω—ñ –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π —ñ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç–∏.\n",
    "\n",
    "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 6: –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è"
   ],
   "id": "42563c86f8b475f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:19:44.942883Z",
     "start_time": "2024-11-28T16:19:43.042775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(tag2id))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ],
   "id": "3c1add46bc1c6106",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Misha\\PycharmProjects\\mountains\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\2908876682.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ –º–æ–¥–µ–ª—å —ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è.\n",
    "\n",
    "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 7: –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"
   ],
   "id": "b5872b5e398058c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:33:57.147291Z",
     "start_time": "2024-11-28T16:19:44.958483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"./bert-ner\")\n"
   ],
   "id": "69d06ccffa2175b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 14:04, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.318423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.250810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.197148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.146047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.102733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.070225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.047477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.031124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.019781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.012491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.008256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.006031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.004522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.003444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.002140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.001045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n",
      "C:\\Users\\Misha\\AppData\\Local\\Temp\\ipykernel_10096\\152637357.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.data.items()}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–¢—Ä–µ–Ω—É—î–º–æ –º–æ–¥–µ–ª—å —ñ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —ó—ó.\n",
    "\n",
    "–ö–ª—ñ—Ç–∏–Ω–∫–∞ 8: –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è"
   ],
   "id": "e011b78d129d8341"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:34:03.911819Z",
     "start_time": "2024-11-28T16:33:57.165327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy import displacy\n",
    "\n",
    "ner_model = pipeline(\"ner\", model=\"./bert-ner\", tokenizer=\"./bert-ner\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Mount Everest is the biggest mountain in the world.\"\n",
    "\n",
    "results = ner_model(text)\n",
    "\n",
    "# –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è spaCy\n",
    "def convert_to_spacy_format(text, results):\n",
    "    entities = []\n",
    "    for entity in results:\n",
    "        start = entity[\"start\"]\n",
    "        end = entity[\"end\"]\n",
    "        label = entity[\"entity_group\"]\n",
    "        entities.append({\"start\": start, \"end\": end, \"label\": label})\n",
    "    return {\"text\": text, \"ents\": entities, \"title\": \"Named Entities\"}\n",
    "\n",
    "# –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏\n",
    "spacy_data = convert_to_spacy_format(text, results)\n",
    "\n",
    "# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é spaCy displacy\n",
    "displacy.render(spacy_data, style=\"ent\", manual=True, jupyter=True)"
   ],
   "id": "7f29fb3f0f9274c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><h2 style=\"margin: 0\">Named Entities</h2>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mount\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LABEL_0</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Everest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LABEL_1</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    is the biggest mountain in the world.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LABEL_0</span>\n",
       "</mark>\n",
       "</div></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –º–æ–¥–µ–ª—å –¥–ª—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è —Å—É—Ç–Ω–æ—Å—Ç–µ–π —ñ –≤—ñ–∑—É–∞–ª—ñ–∑—É—î–º–æ —ó—Ö.\n",
    "\n"
   ],
   "id": "203880fa1a828f62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
